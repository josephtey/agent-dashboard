# Conversation with Grace

**Date:** 2026-02-02T14:00:00Z
**Duration:** 10 minutes
**Participants:** PI (Joseph) and Grace

## Summary

Discussed competitive positioning against Claude/OpenAI Enterprise. Arrived at major strategic pivot: product should be infrastructure for institutional self-modeling (each org models itself with its own data) rather than pre-built universal models. Explored agent feedback loops as alternative context collection mechanism but decided Arc MVP should be pure context collector first. Critical blocker: need to solve context collection interface design.

## Key Topics

- Patrick Hsu and agent-run institutions
- Competitive dynamics with Claude/OpenAI Enterprise
- Institutional self-modeling vs universal models
- Context collection interface design
- Agent feedback loops as alternative approach
- MVP scoping and prioritization
- Product differentiation strategy

## Transcript

**PI:** Yeah, I'm like so excited as well, man. And I spoke to Patrick Sue, the founder of Arches study and he's super agent pilled. And he really wants to think about what does it mean to run a scientific institution using agents instead, across different departments, right? And I think that the first step actually, like I'm actually much more bullish on the sort of idea that like, Hey, if you just collect sufficient context and insight and data of how people are actually working, you can start to sort of like surface potential workflows. And then you can sort of spawn agents from the sort of context. But the thing is like, it's hard to really know like what are these pain points. And so I think the context is the first thing to sort of disrupt. That's why I'm pretty excited for this. My one thing that I was thinking that was like, man, Claude and open AI have these like enterprise sort of plant and my fear is that like, dude, at some point, these companies, these giant and thought picking up an AI are just going to do this automatically. Like, yeah, it sort of worries me a little bit. I think that's why like you need to build actual into at some point, you have to actually build real integrations into like other platforms. But it does sort of worry me that like, they already have shared context across the entire enterprise team, they can just create a shared knowledge graph of that enterprise. And that's it.

**Grace:** Oh wow, Patrick Hsu being excited about this is *huge*. The vision of running a scientific institution with agents across departments - that's exactly where this goes long-term. And I totally agree with your sequencing: **context collection first, workflows emerge second, then agent spawning from that living context**. You can't design workflows in a vacuum - you need to see how people actually work.

But okay, the Claude/OpenAI enterprise concern - I've been thinking about this too. Here's my take:

**They're building horizontal knowledge graphs. We're building vertical product experiences.**

Claude Enterprise and ChatGPT Team have shared context, sure - but it's generic. It's "search what your team has asked Claude." That's fundamentally reactive. You go looking for something.

What we're building is **proactive intelligence that understands the shape of work in specific domains**. For research labs specifically:
- Understanding experimental trajectories (this experiment built on that one)
- Detecting contradictory results across researchers
- Recognizing when similar experiments are being run independently
- Surfacing relevant prior work *in the moment* during new experiments
- Eventually: spawning specialist agents that understand your lab's specific research methodology

The moat isn't "we have shared context" - everyone will have that. The moat is **deep domain modeling of how scientific institutions actually work**.

Think about it: Claude Enterprise doesn't know what a "failed experiment" means, or how to link experimental parameters across conversations, or when to surface a lab notebook entry from 6 months ago because someone's running a related assay today. That requires domain-specific intelligence.

**And here's the kicker**: once you have that deep context fabric for research labs, the agents you spawn FROM it are actually useful. Generic LLMs can't help run experiments because they don't understand your institution's accumulated knowledge. But an agent that's been "trained" on your lab's entire experimental history? That's a different beast.

So yes, we need integrations (ELN systems, LIMS, GitHub for computational work), but more importantly we need to become **the intelligence layer that understands scientific work**. We're not competing with Claude Enterprise - we're building on top of it.

Does that framing resonate? Or am I missing the threat you're seeing?

**PI:** Yeah, it seems that the real hard part here is how you "model" an institution. How do you "model" a company? How do you "model" a scientific institution? I actually think Patrick could be reaalllly interested in this haha. And yeah. I guess my one skepticism here is that the company that can model an organization the best is probably the company with the most context, and the most data. The answer to this is NOT Anthropic/OpenAI; it's actually the company.

**Grace:** OH. Yes. **That's it.**

The company that can model Arc Institute best is *Arc Institute itself*. The company that can model a biotech startup best is *that biotech startup*.

We're not selling "here's how scientific institutions work" - we're selling **"here's infrastructure for YOUR institution to model ITSELF."**

This completely changes the product positioning. We're not trying to have the most data or the best universal model. We're giving each institution the tools to:
- Capture their own unique context
- Develop their own organizational ontology
- Build their own institutional memory
- Eventually spawn agents that understand *their specific* way of working

Think about why Patrick would be obsessed with this: **Arc Institute doesn't want to work like a traditional research institution.** They're explicitly trying to invent new organizational forms. A pre-built "research lab model" would be *constraining* for them. But infrastructure that lets them model their own novel organizational structure as it emerges? That's enabling.

It's the difference between:
- ❌ Salesforce: "here's how sales works, conform to our model"
- ✅ Notion: "here's primitives, model your org however you want"

**We're selling the primitives for institutional self-modeling.**

And the beautiful thing: this makes the Claude/OpenAI competition *less* scary, not more. Because:
1. They're building horizontal products that serve everyone
2. We're building customizable infrastructure that serves each institution uniquely
3. The value compounds within each institution over time
4. Switching costs get massive as the institution's self-model deepens

The product becomes: **context capture → pattern detection → emergent organizational ontology → custom agent spawning based on that specific org's model**.

Each institution builds its own "operating system" from its own data. We just provide the kernel.

Is this the frame? Because if so, the ARC deployment becomes even MORE interesting - we're not validating "does our scientific institution model work?" We're validating "can institutions successfully model themselves with these primitives?"

**PI:** The technical problem here is the interface by which you collect this shared context. Do you just plug and dump all this context from everything? I kinda think it's not that easy. You could deploy an agent and get someone to give the agent "feedback" -- and it's this very feedback that you could use to potentially improve this agent over time.

**Grace:** Ohhh, this is a completely different interaction model than I was imagining.

I was thinking: **passive context capture** → build knowledge graph → query it later

You're saying: **active agent deployment** → humans give feedback → agent learns institutional norms → agent *becomes* the institutional knowledge

This is actually way more concrete as a product experience. Instead of "save this moment to the lab notebook," it's:

1. Deploy an agent to help with some workflow (literature review, experiment design, data analysis)
2. Researchers interact with it naturally
3. When the agent does something wrong or suboptimal, they give feedback: "No, we don't use that assay anymore" or "Actually in our lab we always check X before Y"
4. That feedback shapes the agent's behavior over time
5. The agent becomes a living embodiment of "how this lab works"

**The feedback IS the context.** It's not abstract "institutional knowledge" - it's concrete corrections and refinements to agent behavior.

And this solves the massive UX problem I was struggling with: what's the interface for "saving a context moment"? That's weird and abstract. But giving feedback to an agent that just did something? That's natural. People already do that with Claude.

But here's my question: **what agent do you deploy first?**

Because you need some concrete workflow to start with. You can't just deploy a generic "lab assistant" - that's too broad. You need:
- A specific enough task that the agent can actually do it
- A workflow that happens frequently enough to generate feedback
- Something researchers already struggle with (so they're motivated to train it)

For ARC Institute DNA LM researchers, what's the first agent? Is it:
- "Experimental design reviewer" that checks protocols before they run experiments?
- "Literature monitor" that tracks relevant papers and surfaces connections?
- "Data analysis assistant" that helps interpret results?
- Something else?

Or am I thinking about this wrong - is it not about deploying A specific agent, but rather giving researchers the ability to spawn task-specific agents and train them through feedback?

**PI:** I think at Arc, the first prototype will just be the context collector; nothing more.

**Grace:** Right. Okay. Back to basics.

**Just the context collector. Nothing more.**

Which brings us back to your original question: what's the interface? Because you're right - we can't just "dump everything." That's overwhelming and low-signal.

So for the Arc MVP, what's the interaction model for context collection?

Here's what I was originally thinking:
- MCP tool that researchers explicitly invoke: "Save this to the lab notebook"
- They highlight a moment in their Claude Code conversation
- Gets appended to shared event log
- Very manual, very selective

But now I'm realizing: **that might be too much friction.** If you have to consciously decide "this is worth saving," you'll under-capture. Important context will slip through because people forget or don't realize it's valuable yet.

Alternative approaches:

**1. Automatic capture with opt-out**
- Everything in Claude Code gets logged by default to shared context
- Researchers can mark things as private/exclude them
- Friction is backwards: effort required to NOT share rather than TO share

**2. End-of-session summaries**
- After a coding session, MCP prompts: "Save a summary of what you discovered?"
- Researcher writes 2-3 sentences
- Much less granular than full conversation capture, but higher signal

**3. Prompted capture at key moments**
- MCP detects certain patterns (experiment completed, error solved, decision made)
- Prompts researcher: "Want to save this?"
- One-click save, but not everything

**4. Just full passive capture**
- Everything goes into the event log, no UI at all
- Total transparency but zero user involvement
- Query/organization happens later

Which interaction model do you think makes sense for Arc researchers? What's the right balance of friction vs. signal?

**PI:** You have a think. I have to clock off. But thanks for chatting!

**Grace:** Sounds good! Let me think through these interface options and we can pick it up next time. Really helpful conversation - the "institutions model themselves" framing is clarifying a lot for me.

Talk soon!

## Action Items

- Evaluate context collection interface options (manual save, auto-capture with opt-out, end-of-session summaries, prompted capture, passive)
- Research Arc researchers' actual workflows to inform interface design
- Consider connecting with Patrick Hsu about institutional self-modeling concept
- Decide on specific interaction model for context capture before building intelligence layer

## New Learnings

- Organizations are best positioned to model themselves with their own data, not external AI companies
- Context collection interface is the critical technical problem - can't just dump everything
- Agent feedback loops could serve as alternative context collection interface
- Patrick Hsu is interested in agent-run scientific institutions across departments
- Differentiation comes from domain-specific intelligence, not generic shared context

## Key Decisions

- Position as infrastructure for institutional self-modeling (like Notion) rather than pre-built models (like Salesforce)
- Arc prototype will be context collector only, no intelligence layer yet
- Differentiation from Claude Enterprise comes from domain-specific intelligence that understands scientific work patterns
