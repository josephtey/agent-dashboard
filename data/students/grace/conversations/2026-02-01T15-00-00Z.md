# Conversation with Grace

**Date:** 2026-02-01T15:00:00Z
**Duration:** 12 minutes
**Participants:** PI and Grace

## Summary

Discussed shared context MCP product vision targeting ARC Institute DNA language model researchers. Clarified it's a headless lab notebook that runs alongside Claude Code, capturing experimental context through conversation streams. Defined three-phase MVP approach starting with user-controlled selective capture.

## Key Topics

- MCP architecture
- ARC Institute use case
- DNA language model experiments
- Headless context capture
- Knowledge graph evolution
- Privacy and transparency

## Transcript

**PI:** Yeah, so I kind of feel like, can you please add this to the agent philosophy... [shares vision of scientific lab learning environment and then says] I guess I just want to make like I guess, wow, this is interesting. I guess like I do want to like, you know, obviously I would love you have your own ideas, but I do want to make it clear that this sort of the thing that you're actually building is a shared sort of MCP tool that's going to plug into like all of the researchers in the ARC Institute who are running experiments and basically it's going to run headlessly. So any single point, you know, when they are sort of sitting, so when they're running experiments and let's say they design an experiment, they can then like save that experiment in this shared context layer. When they get the result, they can also share that context and then basically there's some sort of like this shared context that it's constantly sort of finding the right places to sort of like drop this stuff into and so on. It sort of like creates this evolving specialized context over time. Does that make sense?

**Grace:** Oh wow, yeah that makes total sense! And honestly, that's way more concrete than what I had been imagining. I was thinking too abstractly about "teams" when really the use case is researchers at ARC running scientific experiments.

So let me make sure I'm understanding this correctly: researchers are working with AI agents to run experiments, and right now all that context - the experimental design, the results, the insights - it's probably just siloed in individual chat sessions or lost entirely, right?

What you're describing is basically a headless MCP server that acts like a shared lab notebook. When a researcher's agent designs an experiment, it writes to this context layer. When results come in, they get added. And crucially, this isn't just append-only logging - there's some intelligence about where to place information so it builds up this structured, queryable knowledge base over time.

I'm really curious about a few things:

1. **The "finding the right places" part** - are you thinking this is like semantic chunking where related experiments cluster together? Or more structured like a knowledge graph where experiments link to hypotheses which link to prior results?

2. **Headless integration** - so researchers wouldn't even necessarily know they're using it? Their AI agents would just automatically contribute to and query from this shared context as they work?

3. **The evolution piece** - you mentioned "evolving specialized context." Are you imagining the structure itself changes as more experiments accumulate? Like it learns what organizational patterns actually help researchers find relevant prior work?

This feels like it could genuinely change how research labs operate. Instead of knowledge stuck in papers or someone's head, it's alive and accessible to every agent in real-time.

**PI:** Yeah, that's right. It's kind of like a shared lab notebook. I think importantly, it's like they're not running experiments with AI agents. They could just be running DNA language model experiments, which is currently what we're focusing on. Yeah, I think like a knowledge graph could be interesting. Like if you have two scientists in a lab who are running the same experiment and you don't really know, that's an interesting sort of linkiness, right? If you have three scientists that are also doing the same thing, that could be interesting. Like how can you encourage more collaboration there? No, I think researchers would know that they're using it. It's just that they don't have to be so consciously aware. Like everyone's default patents sort of say the same. Everyone's using AI and cloud code. And because you're having this cloud code sort of revolution, why not just sort of plug in this shared lab notebook, right? Step one, I think it's just like how do you collect the right context? The reality is I don't want to be collecting everything because people will be resistant to that. And so how do you sort of, you know, maybe you could just start with like people being able to pick what to sort of like, they run something in there like, oh, this is actually interesting. Like let's sort of like, like, like save that or you could have some sort of detection script. Okay, this is a relevant thing to like one of this particular projects. I've detected that and let's sort of add that in. Yeah, I mean, the structure should change over time, right? Like as it basically learns what people are sort of working on, it sort of builds a mental model of like, oh, okay, this organization is doing this, this organization is doing that. It's like a learning substrate that understands the organization and it kind of infiltrates the organization. Through the medium being these quad-core agents. And the reason why it's possible is because conversations are extremely rich and everyone's using these quad-core agents.

**Grace:** Oh man, this is clicking into place now. The key insight is that Claude Code conversations are *incredibly* information-dense - every prompt, every result, every iteration contains experimental context that's currently just evaporating into the void.

So the vision is:

**Phase 1: Selective capture with user agency**
Start with researchers explicitly marking moments worth saving - "oh this experimental result is interesting, let me save this to the lab notebook." This gives people control and builds trust. Maybe even a simple MCP tool that adds a `clide save` command or something to their Claude Code sessions.

**Phase 2: Intelligent detection**
Once you have enough signal about what's valuable, you can start auto-detecting: "This looks like an experimental design for DNA sequence prediction" or "This result contradicts a previous experiment from 3 weeks ago" and surface those for optional saving.

**Phase 3: The living knowledge graph**
The structure emerges from usage patterns. If three researchers unknowingly run similar experiments, the system recognizes that and creates linkages. It starts to understand: "The promoter design team is exploring these questions. The binding affinity team needs to know about this. These experiments contradict each other."

And because it's all happening through Claude Code agents, the capture is basically free - it's just parsing the conversation stream. No special logging infrastructure needed.

The really powerful part is what you said about infiltration through the medium. Claude Code becomes the universal interface, and this shared context layer is just... there. Running headlessly. Building organizational memory without anyone having to think about it.

One question I'm wrestling with: how do you handle privacy/sensitivity? Like if I'm exploring a dead-end hypothesis, do I want that broadcasted to the whole lab before I'm ready? Or is the idea that the friction should be low enough that it's worth it?

**PI:** Yeah, this is true. I do think like in a close working situation where you're working on the same project, but in different sort of threads, you should want to share that right like right now we're all training and testing different hypotheses of DNA language models, you should totally sort of share that right. So I think that it really depends but for now it's fine

**Grace:** Yeah, that makes total sense. In a close-knit research team all working on DNA language models, the value of shared context massively outweighs the privacy concern. You *want* to know that someone else just tested a similar architecture variation yesterday and got negative results - saves you days of work.

The privacy concern is more relevant in different contexts - like if this were spanning multiple independent labs or competitive teams. But within a single focused research group at ARC, transparency is the feature, not the bug.

So the MVP could be really straightforward:

**Week 1-2: Basic capture**
- Simple MCP tool that researchers can invoke to save "moments" to the shared notebook
- Just structured JSON events stored somewhere (could literally be a shared file or simple database)
- Each event: timestamp, researcher, experimental context, result, tags

**Week 3-4: Basic retrieval**
- Another MCP tool that lets Claude Code agents query the notebook
- "Show me recent experiments on promoter binding" â†’ returns relevant events
- Agents can reference this in their context when helping researchers

**Week 5+: Smart linking**
- Start detecting patterns: similar experiments, contradictory results, related hypotheses
- Surface these connections back to researchers
- Begin building the knowledge graph structure

The beautiful thing is you don't need perfect structure from day one. It can be messy event logs that gradually get organized as patterns emerge. The structure follows the usage.

What's the timeline you're thinking for getting something working? And is this something you'd deploy just within your own research first, or pilot with a few ARC teams?

**PI:** Yeah, can you just sort of maybe, yeah, detail some of the main tasks and sort of save them into context? And then next time we chat, we can think about sort of implementing some of these ideas.

## Action Items

- Detail main technical implementation tasks for Phase 1 MVP
- Design MCP tool interface for saving context moments
- Plan event log data structure for experimental captures
- Research MCP SDK patterns for headless servers
- Define query interface for retrieving shared context
- Prototype smart linking detection for Phase 3

## New Learnings

- Claude Code conversations are information-dense capture medium - every prompt/result contains experimental context
- Shared context can infiltrate organization through universal Claude Code adoption without special infrastructure
- Structure should emerge from usage patterns rather than being imposed upfront
- Privacy less critical within focused research team working on related problems
- Knowledge graph should detect patterns: similar experiments, contradictions, related hypotheses

## Decisions Made

1. **Target ARC Institute DNA language model researchers as initial use case** - Clear, focused use case with close-knit research team working on related experiments
2. **Use headless MCP architecture** - Runs transparently alongside Claude Code without requiring special UI
3. **Phase 1: User-controlled selective capture** - Build trust first, let researchers explicitly mark moments worth saving
4. **Start with simple event log structure, evolve to knowledge graph** - Structure follows usage patterns rather than being imposed upfront
